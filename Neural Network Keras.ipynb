{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('housepricedata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>AboveMedianPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>756</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>953</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1542</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1152</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1256</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LotArea  OverallQual  OverallCond  TotalBsmtSF  FullBath  HalfBath  \\\n",
       "0        8450            7            5          856         2         1   \n",
       "1        9600            6            8         1262         2         0   \n",
       "2       11250            7            5          920         2         1   \n",
       "3        9550            7            5          756         1         0   \n",
       "4       14260            8            5         1145         2         1   \n",
       "...       ...          ...          ...          ...       ...       ...   \n",
       "1455     7917            6            5          953         2         1   \n",
       "1456    13175            6            6         1542         2         0   \n",
       "1457     9042            7            9         1152         2         0   \n",
       "1458     9717            5            6         1078         1         0   \n",
       "1459     9937            5            6         1256         1         1   \n",
       "\n",
       "      BedroomAbvGr  TotRmsAbvGrd  Fireplaces  GarageArea  AboveMedianPrice  \n",
       "0                3             8           0         548                 1  \n",
       "1                3             6           1         460                 1  \n",
       "2                3             6           1         608                 1  \n",
       "3                3             7           1         642                 0  \n",
       "4                4             9           1         836                 1  \n",
       "...            ...           ...         ...         ...               ...  \n",
       "1455             3             7           1         460                 1  \n",
       "1456             3             7           2         500                 1  \n",
       "1457             4             9           2         252                 1  \n",
       "1458             2             5           0         240                 0  \n",
       "1459             3             6           0         276                 0  \n",
       "\n",
       "[1460 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8450,     7,     5, ...,     0,   548,     1],\n",
       "       [ 9600,     6,     8, ...,     1,   460,     1],\n",
       "       [11250,     7,     5, ...,     1,   608,     1],\n",
       "       ...,\n",
       "       [ 9042,     7,     9, ...,     2,   252,     1],\n",
       "       [ 9717,     5,     6, ...,     0,   240,     0],\n",
       "       [ 9937,     5,     6, ...,     0,   276,     0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits our dataset into input features (X) and the feature we wish to predict (Y). We assign the first 10 columns of our array to X and the last column to Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything before the comma = rows of the array, after the comma = columns of the array. We're not splitting the rows so ':'means take all the rows in the dataset and put it in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize our values from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0334198 , 0.66666667, 0.5       , ..., 0.5       , 0.        ,\n",
       "        0.3864598 ],\n",
       "       [0.03879502, 0.55555556, 0.875     , ..., 0.33333333, 0.33333333,\n",
       "        0.32440056],\n",
       "       [0.04650728, 0.66666667, 0.5       , ..., 0.33333333, 0.33333333,\n",
       "        0.42877292],\n",
       "       ...,\n",
       "       [0.03618687, 0.66666667, 1.        , ..., 0.58333333, 0.66666667,\n",
       "        0.17771509],\n",
       "       [0.03934189, 0.44444444, 0.625     , ..., 0.25      , 0.        ,\n",
       "        0.16925247],\n",
       "       [0.04037019, 0.44444444, 0.625     , ..., 0.33333333, 0.        ,\n",
       "        0.19464034]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split our dataset into a training set, a validation set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have 6 variables for our dataset:\n",
    "- X_train (10 input features, 70% of the full dataset)\n",
    "- X_val (10 input features, 15% of the full dataset)\n",
    "- X_test (10 input featrues, 15% of full dataset)\n",
    "- Y_train (1 label, 70% of full dataset)\n",
    "- Y_val (1 label, 15% of full dataset)\n",
    "- Y_test (1 label, 15% of full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 10) (182, 10) (183, 10) (1095,) (182,) (183,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we've processed the data and:\n",
    "- Read in the CSV file and converted them into arrays\n",
    "- Split our dataset into the input features and the label\n",
    "- Scaled the data so the input features have similar orders of magnitude\n",
    "- Split our dataset into the training set, the validation set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1095 samples, validate on 182 samples\n",
      "Epoch 1/300\n",
      "1095/1095 [==============================] - 1s 849us/step - loss: 0.6931 - acc: 0.4941 - val_loss: 0.6862 - val_acc: 0.5055\n",
      "Epoch 2/300\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.6839 - acc: 0.4977 - val_loss: 0.6773 - val_acc: 0.5055\n",
      "Epoch 3/300\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.6742 - acc: 0.5059 - val_loss: 0.6701 - val_acc: 0.5220\n",
      "Epoch 4/300\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.6669 - acc: 0.5580 - val_loss: 0.6646 - val_acc: 0.6264\n",
      "Epoch 5/300\n",
      "1095/1095 [==============================] - 0s 86us/step - loss: 0.6608 - acc: 0.6457 - val_loss: 0.6597 - val_acc: 0.7253\n",
      "Epoch 6/300\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.6551 - acc: 0.7233 - val_loss: 0.6549 - val_acc: 0.7418\n",
      "Epoch 7/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.6493 - acc: 0.7562 - val_loss: 0.6496 - val_acc: 0.7473\n",
      "Epoch 8/300\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.6431 - acc: 0.7680 - val_loss: 0.6436 - val_acc: 0.7802\n",
      "Epoch 9/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.6364 - acc: 0.7881 - val_loss: 0.6377 - val_acc: 0.8022\n",
      "Epoch 10/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.6299 - acc: 0.8018 - val_loss: 0.6314 - val_acc: 0.8077\n",
      "Epoch 11/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.6230 - acc: 0.8046 - val_loss: 0.6251 - val_acc: 0.8132\n",
      "Epoch 12/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.6160 - acc: 0.8073 - val_loss: 0.6186 - val_acc: 0.8132\n",
      "Epoch 13/300\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.6090 - acc: 0.8009 - val_loss: 0.6119 - val_acc: 0.8352\n",
      "Epoch 14/300\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.6017 - acc: 0.8119 - val_loss: 0.6048 - val_acc: 0.8297\n",
      "Epoch 15/300\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.5941 - acc: 0.8119 - val_loss: 0.5977 - val_acc: 0.8352\n",
      "Epoch 16/300\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.5864 - acc: 0.8128 - val_loss: 0.5904 - val_acc: 0.8352\n",
      "Epoch 17/300\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.5785 - acc: 0.8137 - val_loss: 0.5828 - val_acc: 0.8352\n",
      "Epoch 18/300\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.5704 - acc: 0.8164 - val_loss: 0.5751 - val_acc: 0.8407\n",
      "Epoch 19/300\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.5622 - acc: 0.8283 - val_loss: 0.5666 - val_acc: 0.8462\n",
      "Epoch 20/300\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.5531 - acc: 0.8301 - val_loss: 0.5579 - val_acc: 0.8516\n",
      "Epoch 21/300\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.5441 - acc: 0.8228 - val_loss: 0.5490 - val_acc: 0.8516\n",
      "Epoch 22/300\n",
      "1095/1095 [==============================] - 0s 106us/step - loss: 0.5353 - acc: 0.8320 - val_loss: 0.5400 - val_acc: 0.8462\n",
      "Epoch 23/300\n",
      "1095/1095 [==============================] - 0s 88us/step - loss: 0.5259 - acc: 0.8347 - val_loss: 0.5305 - val_acc: 0.8516\n",
      "Epoch 24/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.5165 - acc: 0.8329 - val_loss: 0.5209 - val_acc: 0.8516\n",
      "Epoch 25/300\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.5070 - acc: 0.8338 - val_loss: 0.5112 - val_acc: 0.8516\n",
      "Epoch 26/300\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.4976 - acc: 0.8329 - val_loss: 0.5014 - val_acc: 0.8516\n",
      "Epoch 27/300\n",
      "1095/1095 [==============================] - 0s 88us/step - loss: 0.4885 - acc: 0.8393 - val_loss: 0.4922 - val_acc: 0.8516\n",
      "Epoch 28/300\n",
      "1095/1095 [==============================] - 0s 106us/step - loss: 0.4794 - acc: 0.8393 - val_loss: 0.4830 - val_acc: 0.8626\n",
      "Epoch 29/300\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 0.4703 - acc: 0.8420 - val_loss: 0.4718 - val_acc: 0.8571\n",
      "Epoch 30/300\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.4610 - acc: 0.8438 - val_loss: 0.4620 - val_acc: 0.8571\n",
      "Epoch 31/300\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.4523 - acc: 0.8438 - val_loss: 0.4524 - val_acc: 0.8571\n",
      "Epoch 32/300\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.4440 - acc: 0.8457 - val_loss: 0.4430 - val_acc: 0.8571\n",
      "Epoch 33/300\n",
      "1095/1095 [==============================] - 0s 95us/step - loss: 0.4355 - acc: 0.8475 - val_loss: 0.4337 - val_acc: 0.8571\n",
      "Epoch 34/300\n",
      "1095/1095 [==============================] - 0s 92us/step - loss: 0.4274 - acc: 0.8511 - val_loss: 0.4247 - val_acc: 0.8626\n",
      "Epoch 35/300\n",
      "1095/1095 [==============================] - 0s 78us/step - loss: 0.4199 - acc: 0.8511 - val_loss: 0.4160 - val_acc: 0.8626\n",
      "Epoch 36/300\n",
      "1095/1095 [==============================] - 0s 101us/step - loss: 0.4125 - acc: 0.8502 - val_loss: 0.4091 - val_acc: 0.8736\n",
      "Epoch 37/300\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.4057 - acc: 0.8502 - val_loss: 0.4008 - val_acc: 0.8681\n",
      "Epoch 38/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.3992 - acc: 0.8557 - val_loss: 0.3924 - val_acc: 0.8736\n",
      "Epoch 39/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.3929 - acc: 0.8539 - val_loss: 0.3851 - val_acc: 0.8681\n",
      "Epoch 40/300\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.3870 - acc: 0.8575 - val_loss: 0.3782 - val_acc: 0.8791\n",
      "Epoch 41/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.3812 - acc: 0.8603 - val_loss: 0.3725 - val_acc: 0.8791\n",
      "Epoch 42/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.3756 - acc: 0.8621 - val_loss: 0.3657 - val_acc: 0.8791\n",
      "Epoch 43/300\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.3717 - acc: 0.8603 - val_loss: 0.3598 - val_acc: 0.8846\n",
      "Epoch 44/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.3670 - acc: 0.8612 - val_loss: 0.3544 - val_acc: 0.8791\n",
      "Epoch 45/300\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.3621 - acc: 0.8630 - val_loss: 0.3492 - val_acc: 0.8846\n",
      "Epoch 46/300\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.3582 - acc: 0.8667 - val_loss: 0.3439 - val_acc: 0.8846\n",
      "Epoch 47/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.3536 - acc: 0.8694 - val_loss: 0.3397 - val_acc: 0.8791\n",
      "Epoch 48/300\n",
      "1095/1095 [==============================] - 0s 85us/step - loss: 0.3502 - acc: 0.8648 - val_loss: 0.3355 - val_acc: 0.8791\n",
      "Epoch 49/300\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.3468 - acc: 0.8676 - val_loss: 0.3304 - val_acc: 0.8956\n",
      "Epoch 50/300\n",
      "1095/1095 [==============================] - 0s 106us/step - loss: 0.3435 - acc: 0.8667 - val_loss: 0.3268 - val_acc: 0.8901\n",
      "Epoch 51/300\n",
      "1095/1095 [==============================] - 0s 96us/step - loss: 0.3407 - acc: 0.8639 - val_loss: 0.3227 - val_acc: 0.8846\n",
      "Epoch 52/300\n",
      "1095/1095 [==============================] - 0s 93us/step - loss: 0.3375 - acc: 0.8703 - val_loss: 0.3192 - val_acc: 0.8791\n",
      "Epoch 53/300\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.3348 - acc: 0.8758 - val_loss: 0.3153 - val_acc: 0.8956\n",
      "Epoch 54/300\n",
      "1095/1095 [==============================] - 0s 92us/step - loss: 0.3320 - acc: 0.8703 - val_loss: 0.3121 - val_acc: 0.8956\n",
      "Epoch 55/300\n",
      "1095/1095 [==============================] - 0s 104us/step - loss: 0.3301 - acc: 0.8685 - val_loss: 0.3103 - val_acc: 0.8791\n",
      "Epoch 56/300\n",
      "1095/1095 [==============================] - 0s 87us/step - loss: 0.3274 - acc: 0.8721 - val_loss: 0.3063 - val_acc: 0.8956\n",
      "Epoch 57/300\n",
      "1095/1095 [==============================] - 0s 111us/step - loss: 0.3253 - acc: 0.8694 - val_loss: 0.3035 - val_acc: 0.8846\n",
      "Epoch 58/300\n",
      "1095/1095 [==============================] - 0s 170us/step - loss: 0.3230 - acc: 0.8712 - val_loss: 0.3011 - val_acc: 0.8846\n",
      "Epoch 59/300\n",
      "1095/1095 [==============================] - 0s 92us/step - loss: 0.3207 - acc: 0.8703 - val_loss: 0.2978 - val_acc: 0.8901\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.3183 - acc: 0.8721 - val_loss: 0.2980 - val_acc: 0.9011\n",
      "Epoch 61/300\n",
      "1095/1095 [==============================] - 0s 86us/step - loss: 0.3170 - acc: 0.8703 - val_loss: 0.2952 - val_acc: 0.8956\n",
      "Epoch 62/300\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.3151 - acc: 0.8721 - val_loss: 0.2908 - val_acc: 0.8901\n",
      "Epoch 63/300\n",
      "1095/1095 [==============================] - 0s 80us/step - loss: 0.3131 - acc: 0.8758 - val_loss: 0.2895 - val_acc: 0.8956\n",
      "Epoch 64/300\n",
      "1095/1095 [==============================] - 0s 82us/step - loss: 0.3116 - acc: 0.8740 - val_loss: 0.2868 - val_acc: 0.8956\n",
      "Epoch 65/300\n",
      "1095/1095 [==============================] - 0s 83us/step - loss: 0.3099 - acc: 0.8703 - val_loss: 0.2854 - val_acc: 0.8956\n",
      "Epoch 66/300\n",
      "1095/1095 [==============================] - 0s 89us/step - loss: 0.3081 - acc: 0.8749 - val_loss: 0.2824 - val_acc: 0.8901\n",
      "Epoch 67/300\n",
      "1095/1095 [==============================] - 0s 164us/step - loss: 0.3065 - acc: 0.8740 - val_loss: 0.2860 - val_acc: 0.9011\n",
      "Epoch 68/300\n",
      "1095/1095 [==============================] - 0s 174us/step - loss: 0.3062 - acc: 0.8712 - val_loss: 0.2790 - val_acc: 0.8901\n",
      "Epoch 69/300\n",
      "1095/1095 [==============================] - 0s 115us/step - loss: 0.3039 - acc: 0.8758 - val_loss: 0.2787 - val_acc: 0.9066\n",
      "Epoch 70/300\n",
      "1095/1095 [==============================] - 0s 89us/step - loss: 0.3030 - acc: 0.8758 - val_loss: 0.2756 - val_acc: 0.8956\n",
      "Epoch 71/300\n",
      "1095/1095 [==============================] - 0s 98us/step - loss: 0.3010 - acc: 0.8740 - val_loss: 0.2744 - val_acc: 0.9011\n",
      "Epoch 72/300\n",
      "1095/1095 [==============================] - 0s 84us/step - loss: 0.2995 - acc: 0.8785 - val_loss: 0.2732 - val_acc: 0.9011\n",
      "Epoch 73/300\n",
      "1095/1095 [==============================] - 0s 120us/step - loss: 0.2984 - acc: 0.8767 - val_loss: 0.2717 - val_acc: 0.9066\n",
      "Epoch 74/300\n",
      "1095/1095 [==============================] - 0s 146us/step - loss: 0.2976 - acc: 0.8776 - val_loss: 0.2701 - val_acc: 0.9011\n",
      "Epoch 75/300\n",
      "1095/1095 [==============================] - 0s 126us/step - loss: 0.2960 - acc: 0.8758 - val_loss: 0.2699 - val_acc: 0.9011\n",
      "Epoch 76/300\n",
      "1095/1095 [==============================] - 0s 107us/step - loss: 0.2940 - acc: 0.8749 - val_loss: 0.2676 - val_acc: 0.9066\n",
      "Epoch 77/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.2935 - acc: 0.8795 - val_loss: 0.2662 - val_acc: 0.9066\n",
      "Epoch 78/300\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.2924 - acc: 0.8758 - val_loss: 0.2660 - val_acc: 0.8956\n",
      "Epoch 79/300\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.2918 - acc: 0.8767 - val_loss: 0.2654 - val_acc: 0.9066\n",
      "Epoch 80/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.2905 - acc: 0.8776 - val_loss: 0.2650 - val_acc: 0.9121\n",
      "Epoch 81/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.2899 - acc: 0.8749 - val_loss: 0.2622 - val_acc: 0.9121\n",
      "Epoch 82/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.2888 - acc: 0.8776 - val_loss: 0.2605 - val_acc: 0.9066\n",
      "Epoch 83/300\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.2873 - acc: 0.8776 - val_loss: 0.2613 - val_acc: 0.9066\n",
      "Epoch 84/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2869 - acc: 0.8712 - val_loss: 0.2583 - val_acc: 0.9066\n",
      "Epoch 85/300\n",
      "1095/1095 [==============================] - 0s 73us/step - loss: 0.2853 - acc: 0.8785 - val_loss: 0.2575 - val_acc: 0.9121\n",
      "Epoch 86/300\n",
      "1095/1095 [==============================] - 0s 74us/step - loss: 0.2845 - acc: 0.8822 - val_loss: 0.2565 - val_acc: 0.9121\n",
      "Epoch 87/300\n",
      "1095/1095 [==============================] - 0s 75us/step - loss: 0.2832 - acc: 0.8804 - val_loss: 0.2563 - val_acc: 0.9121\n",
      "Epoch 88/300\n",
      "1095/1095 [==============================] - 0s 69us/step - loss: 0.2833 - acc: 0.8804 - val_loss: 0.2547 - val_acc: 0.9121\n",
      "Epoch 89/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.2821 - acc: 0.8758 - val_loss: 0.2538 - val_acc: 0.9121\n",
      "Epoch 90/300\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.2806 - acc: 0.8795 - val_loss: 0.2531 - val_acc: 0.9121\n",
      "Epoch 91/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2800 - acc: 0.8813 - val_loss: 0.2525 - val_acc: 0.9121\n",
      "Epoch 92/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2795 - acc: 0.8813 - val_loss: 0.2516 - val_acc: 0.9121\n",
      "Epoch 93/300\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.2786 - acc: 0.8813 - val_loss: 0.2511 - val_acc: 0.9121\n",
      "Epoch 94/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.2773 - acc: 0.8822 - val_loss: 0.2574 - val_acc: 0.9176\n",
      "Epoch 95/300\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.2758 - acc: 0.8813 - val_loss: 0.2499 - val_acc: 0.9121\n",
      "Epoch 96/300\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.2752 - acc: 0.8822 - val_loss: 0.2495 - val_acc: 0.9121\n",
      "Epoch 97/300\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.2752 - acc: 0.8831 - val_loss: 0.2535 - val_acc: 0.9121\n",
      "Epoch 98/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2754 - acc: 0.8804 - val_loss: 0.2485 - val_acc: 0.9121\n",
      "Epoch 99/300\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.2738 - acc: 0.8795 - val_loss: 0.2483 - val_acc: 0.9121\n",
      "Epoch 100/300\n",
      "1095/1095 [==============================] - 0s 77us/step - loss: 0.2729 - acc: 0.8813 - val_loss: 0.2473 - val_acc: 0.9121\n",
      "Epoch 101/300\n",
      "1095/1095 [==============================] - 0s 83us/step - loss: 0.2720 - acc: 0.8804 - val_loss: 0.2480 - val_acc: 0.9121\n",
      "Epoch 102/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.2718 - acc: 0.8849 - val_loss: 0.2474 - val_acc: 0.9121\n",
      "Epoch 103/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2703 - acc: 0.8831 - val_loss: 0.2458 - val_acc: 0.9121\n",
      "Epoch 104/300\n",
      "1095/1095 [==============================] - 0s 79us/step - loss: 0.2699 - acc: 0.8804 - val_loss: 0.2482 - val_acc: 0.9121\n",
      "Epoch 105/300\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.2701 - acc: 0.8804 - val_loss: 0.2481 - val_acc: 0.9066\n",
      "Epoch 106/300\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.2693 - acc: 0.8813 - val_loss: 0.2450 - val_acc: 0.9176\n",
      "Epoch 107/300\n",
      "1095/1095 [==============================] - 0s 65us/step - loss: 0.2681 - acc: 0.8840 - val_loss: 0.2445 - val_acc: 0.9066\n",
      "Epoch 108/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2682 - acc: 0.8849 - val_loss: 0.2435 - val_acc: 0.9121\n",
      "Epoch 109/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.2661 - acc: 0.8849 - val_loss: 0.2441 - val_acc: 0.9121\n",
      "Epoch 110/300\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.2669 - acc: 0.8813 - val_loss: 0.2437 - val_acc: 0.9121\n",
      "Epoch 111/300\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.2667 - acc: 0.8831 - val_loss: 0.2426 - val_acc: 0.9176\n",
      "Epoch 112/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.2649 - acc: 0.8831 - val_loss: 0.2416 - val_acc: 0.9176\n",
      "Epoch 113/300\n",
      "1095/1095 [==============================] - 0s 76us/step - loss: 0.2647 - acc: 0.8840 - val_loss: 0.2411 - val_acc: 0.9066\n",
      "Epoch 114/300\n",
      "1095/1095 [==============================] - 0s 70us/step - loss: 0.2648 - acc: 0.8840 - val_loss: 0.2413 - val_acc: 0.9121\n",
      "Epoch 115/300\n",
      "1095/1095 [==============================] - 0s 71us/step - loss: 0.2644 - acc: 0.8822 - val_loss: 0.2410 - val_acc: 0.9121\n",
      "Epoch 116/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.2637 - acc: 0.8840 - val_loss: 0.2401 - val_acc: 0.9176\n",
      "Epoch 117/300\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.2626 - acc: 0.8831 - val_loss: 0.2402 - val_acc: 0.9121\n",
      "Epoch 118/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2622 - acc: 0.8822 - val_loss: 0.2389 - val_acc: 0.9176\n",
      "Epoch 119/300\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.2611 - acc: 0.8840 - val_loss: 0.2437 - val_acc: 0.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "1095/1095 [==============================] - 0s 63us/step - loss: 0.2619 - acc: 0.8858 - val_loss: 0.2382 - val_acc: 0.9176\n",
      "Epoch 121/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2610 - acc: 0.8804 - val_loss: 0.2411 - val_acc: 0.9121\n",
      "Epoch 122/300\n",
      "1095/1095 [==============================] - 0s 61us/step - loss: 0.2602 - acc: 0.8868 - val_loss: 0.2374 - val_acc: 0.9231\n",
      "Epoch 123/300\n",
      "1095/1095 [==============================] - 0s 67us/step - loss: 0.2592 - acc: 0.8840 - val_loss: 0.2440 - val_acc: 0.9121\n",
      "Epoch 124/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2589 - acc: 0.8831 - val_loss: 0.2439 - val_acc: 0.9121\n",
      "Epoch 125/300\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.2599 - acc: 0.8840 - val_loss: 0.2390 - val_acc: 0.9121\n",
      "Epoch 126/300\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.2595 - acc: 0.8849 - val_loss: 0.2362 - val_acc: 0.9176\n",
      "Epoch 127/300\n",
      "1095/1095 [==============================] - 0s 68us/step - loss: 0.2575 - acc: 0.8849 - val_loss: 0.2368 - val_acc: 0.9121\n",
      "Epoch 128/300\n",
      "1095/1095 [==============================] - 0s 72us/step - loss: 0.2579 - acc: 0.8858 - val_loss: 0.2382 - val_acc: 0.9121\n",
      "Epoch 129/300\n",
      "1095/1095 [==============================] - 0s 64us/step - loss: 0.2563 - acc: 0.8904 - val_loss: 0.2412 - val_acc: 0.9121\n",
      "Epoch 130/300\n",
      "1095/1095 [==============================] - 0s 66us/step - loss: 0.2575 - acc: 0.8858 - val_loss: 0.2417 - val_acc: 0.9121\n",
      "Epoch 131/300\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.2570 - acc: 0.8858 - val_loss: 0.2350 - val_acc: 0.9176\n",
      "Epoch 132/300\n",
      "1095/1095 [==============================] - 0s 60us/step - loss: 0.2553 - acc: 0.8904 - val_loss: 0.2375 - val_acc: 0.9121\n",
      "Epoch 133/300\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.2560 - acc: 0.8868 - val_loss: 0.2353 - val_acc: 0.9176\n",
      "Epoch 134/300\n",
      "1095/1095 [==============================] - 0s 62us/step - loss: 0.2548 - acc: 0.8840 - val_loss: 0.2340 - val_acc: 0.9176\n",
      "Epoch 135/300\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.2558 - acc: 0.8858 - val_loss: 0.2342 - val_acc: 0.9231\n",
      "Epoch 136/300\n",
      "1095/1095 [==============================] - 0s 58us/step - loss: 0.2547 - acc: 0.8868 - val_loss: 0.2347 - val_acc: 0.9121\n",
      "Epoch 137/300\n",
      "1095/1095 [==============================] - 0s 59us/step - loss: 0.2546 - acc: 0.8877 - val_loss: 0.2333 - val_acc: 0.9176\n",
      "Epoch 138/300\n",
      " 928/1095 [========================>.....] - ETA: 0s - loss: 0.2471 - acc: 0.8879"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "                batch_size=32, epochs = 300,\n",
    "                validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,Y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
